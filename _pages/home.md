---
layout: project
urltitle:  "Scientific Models and Machine Learning (SMMaL)"
title: "Scientific Models and Machine Learning (SMMaL)"
categories: nips, neurips, vancouver, canada, workshop, mathematics, machine learning, smal, 2019, neurips19
permalink: /
<!-- favicon: /static/img/embodiedqa/favicon.png -->
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>The Symbiosis of Deep Learning and Differential Equations (DLDE)</h1></center>
    <center><h2>NeurIPS 2021 Workshop</h2></center>
    <center><span style="color:#e74c3c;font-weight:400;">
      Tuesday, 14th December, 08:00 AM to 06:30 PM,
      Virtual Workshop
    </span></center>
  </div>
</div>

<hr>

<!-- <div class="row" id="intro"> -->
  <!-- <div class="col-md-12"> -->
    <!-- <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}"> -->
    <!-- <p> Image credit: [2, 28, 12, 11, 15-21, 26]</p> -->
  <!-- </div> -->
<!-- </div> -->

<br>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
<p>

<p>
The focus of this workshop is on the interplay between deep learning (DL) and differential equations (DEs).
In recent years, there has been a rapid increase of machine learning applications in computational sciences, with some of the most impressive results at the interface of DL and DEs. 
These successes have widespread implications, as DEs are among the most well-understood tools for the mathematical analysis of scientific knowledge, and they are fundamental building blocks for mathematical models in engineering, finance, and the natural sciences. 
This relationship is mutually beneficial.
DL techniques have been used in a variety of ways to dramatically enhance the effectiveness of DE solvers and computer simulations.
Conversely, DEs have also been used as mathematical models of the neural architectures and training algorithms arising in DL.</p>

<p>
This workshop will aim to bring together researchers from each discipline to encourage intellectual exchanges and cultivate relationships between the two communities.
The scope of the workshop will include important topics at the intersection of DL and DEs, such as:</p>


<div class="row">
  <div class="col-xs-12">
    <ul>
      <li>How can DE models provide insights into DL?
      <ul>
        <li>What families of functions are best represented by different neural architectures?</li>
        <li>Can this connection guide the design of new neural architectures?</li>
        <li>Can DE models be used to derive bounds on generalization error?</li>
        <li>What insights can DE models provide into training dynamics?</li>
        <li>Can these insights guide the design of weight initialization schemes?</li>
      </ul>
      </li>
      <li>How can DL be used to enhance the analysis of DEs?
      <ul>
        <li>Solving high dimensional DEs (e.g., many-body physics, multi-agent models, …)</li>
        <li>Solving highly parameterized DEs </li>
        <li>Solving inverse problems</li>
        <li>Solving DEs with irregular solutions (exhibiting e.g., singularities, shocks, …)</li>
      </ul>
      </li>
    </ul>
  </div>
</div>



<p>
We expect to publish around 40 articles, out of 80 submissions, and 200 attendees. We plan to crowdsource revisions so every main author to submit an article is expected to review 3-5 submissions.
</p>

<br>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>September 17th,  2021 - Midnight Pacific Time</td>
        </tr>
        <tr>
          <td>Final Decisions</td>
          <td>October 17th, 2021 </td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>December 14th, 2021</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      We invite high-quality paper submissions on the following topics:
    </p>

  <p>
  <ul>
  <li>novel ways to use AI to understand mathematical models</li>
  <li>novel ways to use scientific models to understand AI</li>
  <li>stochastic differential equations to model gradient flow</li>
  <li>deep learning to solve long standing physics problems</li>
  <li>deep learning for high-dimensional Partial Differential Equations</li>
</ul>
    </p>


  <p> <span style="font-weight:500;">Submission:</span>
  <br/>
    Accepted papers will be presented during joint poster sessions. 
    Exceptional submissions will be either selected for four 15-minute contributed talks
    or 8 5-minute spotlight oral presentations. 
    Accepted papers will be made publicly available as non-archival reports,
    allowing future submissions to archival conferences or journals.
  </p>

  <p>
  Submissions should be up to 4 pages excluding references, acknowledgements, and supplementary material, and should be
      <span style="color:#1a1aff;font-weight:400;"><a href="https://nips.cc/Conferences/2018/PaperInformation/StyleFiles">NIPS format</a></span> and anonymous. The review process is double-blind.
  </p>
  <p>
  We also welcome published papers that are within the scope of the workshop (without re-formatting). This specific papers do not have to be anonymous. They are eligible for poster sessions and will only have a very light review process.
  </p>

  <p>
  Please submit your paper to the following address: <a href="https://cmt3.research.microsoft.com/VIGIL2018">https://cmt3.research.microsoft.com/VIGIL2018</a>
  If you have any question, send an email to: vigilworkshop2018@gmail.com
  </p>

  <p><b>Accepted workshop papers are eligible to the pool of reserved conference tickets (one ticket per accepted papers).</b>
  </p>

  </div>

</div><br>




<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <ul>
      <li>08:30 AM : Opening Remarks                        </li>
      <br>
      <p><b>Scientific Models to understand Deep Learning</b></p>
      <li>08:40 AM : Invited Speaker 1: Surya Ganguli - An analytic theory of generalization dynamics and transfer learning in deep linear networks  | <a href="https://google.com">Slides (still unavailable)</a> </li>
      <li>09:20 AM : Invited Speaker 2: Terrence Tao - On the universality of the incompressible Euler equation on compact manifolds, II. Non-rigidity of Euler flows   | <a href="https://google.com">Slides (still unavailable)</a> </li>
      <li>10:00 AM :  </li>
      <li>10:15 AM : Spotlights (6*2min)
      <ul>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
        <!-- <li></li> -->
      </ul>
      </li>
      <li>10:30 AM : Coffee Break (20min)                   </li>
      <li>10:50 AM : Invited Speaker 3: Aasa Feragen - Geometry and Statistics: Manifolds and Stratified Spaces  | <a href="https://google.com">Slides (still unavailable)</a>  </li>
      <li>11:30 AM : Invited Speaker 4: Hava Siegelmann - Analog computation via neural networks | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>12:10 PM : Poster Session <b>Lunch provided!</b> </li>
      <li>01:10 PM : Break                                  </li>
      <br>
      <p><b>Deep Learning to understand Scientific Models</b></p>
      <li>01:40 PM : Invited Speaker 5: Konstantinos Spiliopoulos - DGM: A deep learning algorithm for solving partial differential equations  | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>02:20 PM : Invited Speaker 6: Matthias Troyer - DGM: A deep learning algorithm for solving partial differential equations.      | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>03:00 PM : Coffee Break & Poster Session (50 mins)</li>
      <li>03:50 PM : Invited Speaker 7: Weinan E - DeePMD-kit: A deep learning package for many-body potential energy representation and molecular dynamics   | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>04:30 PM : Invited Speaker 8: Amy Greenwald - Learning Equilibria of Simulation-Based Games | <a href="https://google.com">Slides (still unavailable)</a></li>
      <li>05:10 PM : Panel Discussion                       </li>
      <li>06:00 PM : Closing Remarks                        </li>
    </ul>
  </div>
</div>


<br>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>


<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/weinane.jpg" | prepend:site.baseurl }}">
    <p><b>Weinan E</b>
    is Professor in Applied and Computational Mathematics at Princeton University.
    He has worked on various disciplines of sciences and has contributed to the resolution of some long standing scientific problems such as the Burgers turbulence problem, the Cauchy-Born rule for crystalline solids, and the moving contact line problem. He is interested in bringing clarity to scientific issues through mathematics and in multi-scale and/or multi-physics problems. He has also worked on building the mathematical framework and finding effective numerical algorithms for modeling rare events and has developed diverse tools to analyze algorithms.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://web.math.princeton.edu/~weinan/">Webpage</a>]</span></p>
  </div>
</div><br>



<div class="row" id="recordings">
    <div class="col-xs-12">
    <h2>Recordings</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <p>
    The workshop was broadcasted via BlueJeans. You can find the recordings here: 
    <a href="https://bluejeans.com/s/KAsSh">Morning Session</a> | <a href="https://bluejeans.com/s/hEo5B">Afternoon Session</a></p>
  </div>
</div>

<div class="row" id="accepted-papers">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row">
<div class="col-xs-12">
  <ul>
    <li></li>
    <li></li>
    <li></li>

  </ul>
</div>
</div>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  
  <div class="col-xs-3">
    <a href="http://rlily.hu/">
      <img class="people-pic" src="{{ "/static/img/people/Lily.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://rlily.hu/">Lily Hu</a>
      <h6>Salesforce Research</h6>
      <h6>University of California, Berkeley</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://thorjonsson.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/thor.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://thorjonsson.github.io/">Thor Jonsson</a>
      <h6>University of Guelph</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://lucehe.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/lucaherrtti.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://lucehe.github.io/">Luca Herrtti</a>
      <h6>Université de Sherbrooke</h6>
    </div>
  </div>
  
  <div class="col-xs-3">
    <a href="https://www.ece.rutgers.edu/jorge-ortiz">
      <img class="people-pic" src="{{ "/static/img/people/jortiz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ece.rutgers.edu/jorge-ortiz">Jorge Ortiz</a>
      <h6>Rutgers, The State University of New Jersey</h6>
    </div>
  </div>
  </div>

<hr>



<div class="row">
  <div class="col-xs-12">
    <h2>Sponsors</h2>
  </div>
</div>
<a name="/sponsors"></a>
<div class="row">
  <div class="col-xs-12 sponsor">
    <a href="https://merl.com/">
      <img src="{{ "/static/img/sponsors/merl-logo-big.jpg" | prepend:site.baseurl }}">
    </a>
    <a href="https://deepmind.com/">
      <img src="{{ "/static/img/sponsors/deepmind_logo.png" | prepend:site.baseurl }}">
    </a>
        <a href="https://ai.google/">
      <img src="{{ "/static/img/sponsors/googlelogo.png" | prepend:site.baseurl }}">
    </a>
        <a href="https://iglu-chistera.github.io/">
      <img src="{{ "/static/img/sponsors/logoIGLU-300.png" | prepend:site.baseurl }}">
    </a>
    <a href="http://uber.ai/">
      <img src="{{ "/static/img/sponsors/logo_uber.jpg" | prepend:site.baseurl }}">
    </a>
  </div>
</div>
<br>


<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> and
      <span style="color:#1a1aff;font-weight:400;"> <a href="nips2018vigil.github.io">nips2018vigil.github.io</a></span>
      for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <a id="note1" href="#note1ref">[1]</a> Q. Li, C. Tai and W. E 
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/pdf/1511.06251.pdf">Stochastic modified equations and adaptive stochastic gradient algorithms.</a></span> 
  ICML, 2017.

  <br><a id="note1" href="#note1ref">[2]</a> P. Chaudhari and S. Soatto  
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/pdf/1710.11029.pdf">Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks.</a></span> 
  ICLR, 2018.

  <br><a id="note3" href="#note3ref">[3]</a> G. S. Dhillon1, K. Azizzadenesheli, Z. C. Lipton, J. Bernstein, J. Kossaifi, A. Khanna, A. Anandkumar
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://openreview.net/pdf?id=H1uR4GZRZ">Stochastic Activation Pruning for Robust Adversarial Defense.</a></span> 
  ICLR, 2018.
  
  

  
  <br><a id="note4" href="#note4ref">[4]</a> H.T. Siegelmann and E.D. Sontag
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://pdf.sciencedirectassets.com/272574/1-s2.0-S0022000000X00360/1-s2.0-S0022000085710136/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEIP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIG7N1TvjUHLWQrdS8tQz3KTq2a1Tj4xGM58709NGZjTUAiEAg9EROQsxFp%2F%2FaSRR2CLGjGFF3af76eh64gnUnO%2B14Ukq4wMIrP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDGcHqSFyN5NbhLF2JCq3A49Xw3XDjCOBugnZhO0PdiVRwbLwWrOpy8ErrabGYltqbK7D0ayZCutvH4OHhW1iflrPgatXtJsp1iEnCPj%2BxvbS%2Fc2zsOGDEdxyEMiE47un2GXKovQOTrwmqox8fEuFK0Ip6cLby%2F4fPscc5FV9uTwEyUcbX6dMEcIdeJW1VvAKZPucFX6xHQYDDPfis8Rb3vw8%2F6RjpVI4kLrp%2FdyYzL9UgjwrAWMhelbkPqbd7qnCXHBQTAWdMox5DFuJy2bWfuLpm%2FYTJgRTb2JNvO%2BkgaHrdhoEpGCANq5XdVN8T3COo4%2B8SXAg1X%2FcyMf5JD0nsW1We6myTDnF%2FDoub4UQIN3l4loxC1dwz9dyWBWODknHdzlDPagu%2FiRJfgwIYwVcKgYNaa7a0NlcFPAOgTuKb1IHSClmlfnQLPOuIfgys9ckGcsZ%2FZnhESM4DaozCeqj%2FnPx0NtSotfquXVJoXkS9h7S1WG1nJ4ZZ%2BaNsQLbmV3dVZoyNqPNLkzSTBE8fNNLspqSxRd9ZQm6dXeJYbTGEFP3azqZ86BtSpxNaUEG1bYyp5hCO%2BUHHT5LxE05Iub5GgksL2WvcQQw7O3%2F5wU6tAFyMBLu0kulw3zYCyB9qOF0%2FuhAUXfead08i%2BZAcafQKS92boEElJN5cUop0NXOnsdnoZNrecaFhwUptgBrrtYPOwnLKs3ifhucSrTg3xG%2BHEOF%2BLnB4qGmslweCS2UHAwoPxUXgCMNWQaWTFAhNnpMxDbAdtGpI7tOc2pmZbF1kiR%2Bk%2BZFXqWyeoN649nKg41xJbBF9XbcsR16nKN%2BiQgV9%2F3VNcL4Up9Wp0NUCiBHMEsZ8Yg%3D&AWSAccessKeyId=ASIAQ3PHCVTY24J2UZLE&Expires=1560280614&Signature=2EA9ajQ7E7lxnKGcVfOUEIznF2Q%3D&hash=ea8753eb8f32e979b698ea2e36e2cf36a9f1b28926044de04b6a889359131625&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0022000085710136&tid=spdf-ae0b997b-814f-400f-8aba-4ae0a00dc595&sid=406effae72007249516a78029a955bfad9dfgxrqa&type=client">On the Computational Power of Neural Nets.</a></span> 
  Science, 1995.
  
  <br><a id="note5" href="#note5ref">[5]</a> H.T. Siegelmann and E.D. Sontag
  <span style="color:#1a1aff;font-weight:400;"> <a href="http://www.sontaglab.org/FTPDIR/siegelmann_sontag_tcs1994.pdf">Analog computation via neural networks.</a></span> 
  Science, 1994.  
  
  
  
  
  
  
  
  
  <br><a id="note6" href="#note6ref">[6]</a> G. Carleo and M. Troyer
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://science.sciencemag.org/content/355/6325/602">Solving the Quantum Many-Body Problem with Artificial Neural Networks.</a></span> 
  Science, 2017.
  
  <br><a id="note7" href="#note7ref">[7]</a> J. Sirignano and K. Spiliopoulos  
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://www.sciencedirect.com/science/article/pii/S0021999118305527">DGM: A deep learning algorithm for solving partial differential equations.</a></span>
  Science, 2018.
  
  <br><a id="note8" href="#note8ref">[8]</a> R. Porotti, D. Tamascelli, M. Restelli, E. Prati 
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/abs/1901.06603">Coherent Transport of Quantum States by Deep Reinforcement Learning.</a></span>
  ArXiv, 2019.

  <br>[9] T. Wu, M. Tegmark
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/pdf/1810.10525.pdf">Toward an AI Physicist for Unsupervised Learning.</a></span>
  ArXiv, 2018.

  <br>[10] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://openreview.net/pdf?id=H1W1UN9gg">Deep Information Propagation.</a></span>
  ICLR, 2017.


  <br>[11] Z. Nado, J. Snoek, B. Xu, R. Grosse, D. Duvenaud, and J. Martens
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://openreview.net/pdf?id=ry-Se9kvG
">Stochastic Gradient Langevin Dynamics that Exploit Neural Network Structure.</a></span>
  ICLR, 2018.

  <br>[12] A. Karpatne, G. Atluri, J. H. Faghmous, M. Steinbach, A. Banerjee, A. Ganguly, S. Shekhar, N. Samatova, and V. Kumar
  <span style="color:#1a1aff;font-weight:400;"> <a href="https://arxiv.org/pdf/1612.08544.pdf">Theory-guided Data Science: A New Paradigm for Scientific Discovery from Data.</a></span>
  IEEE Transactions on Knowledge and Data Engineering, 2017.


</div>

<br><br><br><br><br><br><br>



